--- deps.py
+++ deps.py
@@ -60,24 +60,24 @@
         self.device = device
         self.move_map = {
             0: (-1, 0),  # up
             1: (1, 0),   # down
             2: (0, -1),  # left
             3: (0, 1),   # right
         }
-        self.grid = torch.zeros((self.num_channels, GRID_SIZE, GRID_SIZE), dtype=torch.int8, device=self.device)
+        self.grid = torch.zeros((self.num_channels, GRID_SIZE, GRID_SIZE), dtype=torch.float32, device=self.device)
         self.visit_count = torch.zeros((GRID_SIZE, GRID_SIZE), dtype=torch.int16, device=self.device)
         self.reset()

     def reset(self, maze_difficulty=0.5, dist_to_end=0.2):
         """Reset the environment with the given difficulty."""
-        generate_maze_tensor(self.grid, device=self.device, nchannels=self.num_channels, size=GRID_SIZE, difficulty=maze_difficulty)
+        # Generate maze as int8 then convert to float32 once
+        maze_tensor = torch.zeros((self.num_channels, GRID_SIZE, GRID_SIZE), dtype=torch.int8, device=self.device)
+        generate_maze_tensor(maze_tensor, device=self.device, nchannels=self.num_channels, size=GRID_SIZE, difficulty=maze_difficulty)
+        self.grid = maze_tensor.float()  # Convert to float32 once

         agentr = int(GRID_SIZE*(1 - nprand.rand()*dist_to_end))
         agentc = int(GRID_SIZE*(1 - nprand.rand()*dist_to_end))
         self.agent_pos = (agentr, agentc)

         self.grid[1, 0, 0] = 1
         self.goal_pos = (GRID_SIZE - 1, GRID_SIZE - 1)
         self.steps = 0
         self.visit_count.zero_()
@@ -221,27 +221,26 @@
         """Store a transition in the buffer."""
         if len(self.buffer) < self.capacity:
             self.buffer.append(None)
             
         # Ensure action, reward, and done are tensors
         if not isinstance(action, torch.Tensor):
             action = torch.tensor([action], device=self.device)
         if not isinstance(reward, torch.Tensor):
             reward = torch.tensor([reward], dtype=torch.float, device=self.device)
         if not isinstance(done, torch.Tensor):
             done = torch.tensor([done], dtype=torch.bool, device=self.device)
+
+        # Pre-convert states to float32 here, once
+        if state.dtype != torch.float32:
+            state = state.float()
+        if next_state.dtype != torch.float32:
+            next_state = next_state.float()
             
         self.buffer[self.position] = (state, action, reward, next_state, done)
         self.position = (self.position + 1) % self.capacity

     def sample(self, batch_size):
         """Sample a batch of transitions from the buffer."""
         batch_indices = nprand.choice(len(self.buffer), min(batch_size, len(self.buffer)), replace=False)
         batch = [self.buffer[i] for i in batch_indices]

         state, action, reward, next_state, done = zip(*batch)
         
-        # Stack the tensors and convert states to float32
-        state_batch = torch.cat([s.unsqueeze(0) for s in state], dim=0).float()
+        # States are already float32, just stack them
+        state_batch = torch.cat([s.unsqueeze(0) for s in state], dim=0)
         action_batch = torch.cat([a for a in action], dim=0)
         reward_batch = torch.cat([r for r in reward], dim=0)
-        next_state_batch = torch.cat([s.unsqueeze(0) for s in next_state], dim=0).float()
+        next_state_batch = torch.cat([s.unsqueeze(0) for s in next_state], dim=0)
         done_batch = torch.cat([d for d in done], dim=0)
         
@@ -263,7 +262,7 @@
             return nprand.randint(self.num_actions)
         else:
             with torch.no_grad():
-                q_values = self.policy_net(state.unsqueeze(0).float())
+                q_values = self.policy_net(state.unsqueeze(0))  # No need to convert, already float32
                 return q_values.max(1)[1].item()
